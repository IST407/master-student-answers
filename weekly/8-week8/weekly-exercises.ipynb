{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Exploring Different Optimizers in Logistic Regression\n",
    "\n",
    "## Objective\n",
    "In this exercise, you will explore different types of optimizers (e.g., LBFGS, SAG) in logistic regression using multi-dimensional data with features at different scales. You'll observe how different algorithms perform on this dataset without any preprocessing.\n",
    "\n",
    "## Dataset\n",
    "We'll create a synthetic dataset with the following characteristics:\n",
    "- Multiple dimensions\n",
    "- Features at different scales\n",
    "- At least one exponentially distributed feature\n",
    "- At least one categorical feature\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Generate the dataset as described above.\n",
    "2. Use an OrdinalEncoder for the categorical data.\n",
    "3. Split the data into training and test sets.\n",
    "4. Implement logistic regression with different optimizers:\n",
    "   - LBFGS\n",
    "   - SAGA\n",
    "   - SAG (Stochastic Average Gradient)\n",
    "5. Compare the performance of each optimizer.\n",
    "\n",
    "## Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score with sag: 0.8015\n",
      "Accuracy score with saga: 0.8\n",
      "Accuracy score with lbfgs: 0.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeintron/Dropbox/@TEACHING/2024/FALL/teaching-venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jeintron/Dropbox/@TEACHING/2024/FALL/teaching-venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/jeintron/Dropbox/@TEACHING/2024/FALL/teaching-venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "# Numeric features\n",
    "feature1 = np.random.normal(0, 1, n_samples)\n",
    "feature2 = np.random.exponential(2, n_samples)\n",
    "feature3 = np.random.uniform(0, 10000, n_samples)\n",
    "\n",
    "# Categorical feature\n",
    "categories = ['A', 'B', 'C', 'D']\n",
    "feature4 = np.random.choice(categories, n_samples)\n",
    "\n",
    "# Combine features\n",
    "X = np.column_stack((feature1, feature2, feature3, feature4))\n",
    "y = (0.2 * feature1 + np.log(feature2) + 0.0001 * feature3 + np.where(feature4 == 'A', 1, 0) > 2).astype(int)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3', 'feature4'])\n",
    "df['target'] = y\n",
    "\n",
    "# Encode categorical feature\n",
    "encoder = OrdinalEncoder()\n",
    "df['feature4'] = encoder.fit_transform(df[['feature4']])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Implement logistic regression with different optimizers\n",
    "# Hint: Use LogisticRegression(solver='lbfgs'), LogisticRegression(solver='saga'), and LogisticRegression(solver='sag')\n",
    "\n",
    "for solver in ['sag','saga','lbfgs']:\n",
    "\n",
    "    lr = LogisticRegression(solver=solver)\n",
    "\n",
    "    # TODO: Compare the performance of each optimizer\n",
    "    # Hint: Use accuracy_score and log_loss to evaluate performance\n",
    "\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(f\"Accuracy score with {solver}: {accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Questions to Consider\n",
    "\n",
    "1. Which optimizer performs best on this dataset? Why do you think that is?\n",
    "2. How does the performance vary between the training and test sets for each optimizer?\n",
    "3. Do you notice any issues with convergence for any of the optimizers?\n",
    "4. How might the different scales of the features be affecting the performance of each optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Preprocessing and Pipeline Optimization\n",
    "\n",
    "## Objective\n",
    "Building on Exercise 2, you will now implement preprocessing steps and explore how they interact with different optimization algorithms. You'll use your knowledge of pipelines to create an efficient workflow.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Using the same dataset from Exercise 2, create a preprocessing pipeline that includes:\n",
    "   - Handling of categorical variables (e.g., one-hot encoding)\n",
    "   - Scaling of numerical features\n",
    "   - Any other preprocessing steps you think might be beneficial\n",
    "2. Implement this preprocessing pipeline along with logistic regression using different optimizers.\n",
    "3. Compare the performance of each optimizer with and without preprocessing.\n",
    "4. Experiment with different preprocessing techniques and observe their effects on model performance.\n",
    "\n",
    "## Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Use the same data generation code from Exercise 2\n",
    "\n",
    "# TODO: Create a preprocessing pipeline\n",
    "# Hint: Use ColumnTransformer to apply different preprocessing to numerical and categorical columns\n",
    "\n",
    "\n",
    "# TODO: Create a full pipeline that includes preprocessing and logistic regression\n",
    "# Hint: Use Pipeline to combine preprocessing with LogisticRegression\n",
    "\n",
    "# TODO: Implement the pipeline with different optimizers and compare their performance\n",
    "\n",
    "# TODO: Experiment with different preprocessing techniques\n",
    "# For example, try StandardScaler vs MinMaxScaler, or try polynomial features\n",
    "# Hint: Try using the LogTransform class we developed in class\n",
    "\n",
    "# TODO: Compare the performance of each pipeline configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with preprocessing:\n",
      "\n",
      "Optimizer: lbfgs\n",
      "train_accuracy: 0.9830\n",
      "test_accuracy: 0.9865\n",
      "train_log_loss: 0.0431\n",
      "test_log_loss: 0.0414\n",
      "\n",
      "Optimizer: sag\n",
      "train_accuracy: 0.9830\n",
      "test_accuracy: 0.9870\n",
      "train_log_loss: 0.0431\n",
      "test_log_loss: 0.0415\n",
      "\n",
      "Optimizer: saga\n",
      "train_accuracy: 0.9830\n",
      "test_accuracy: 0.9870\n",
      "train_log_loss: 0.0431\n",
      "test_log_loss: 0.0415\n",
      "\n",
      "Results with MinMaxScaler:\n",
      "Test accuracy: 0.9820\n",
      "Test log loss: 0.0490\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Generate synthetic data (same as Exercise 2)\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "feature1 = np.random.normal(0, 1, n_samples)\n",
    "feature2 = np.random.exponential(1, n_samples)\n",
    "feature3 = np.random.uniform(0, 100, n_samples)\n",
    "categories = ['A', 'B', 'C', 'D']\n",
    "feature4 = np.random.choice(categories, n_samples)\n",
    "\n",
    "X = np.column_stack((feature1, feature2, feature3, feature4))\n",
    "y = (0.2 * feature1 + np.log(feature2) + 0.01 * feature3 + np.where(feature4 == 'A', 1, 0) > 2).astype(int)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3', 'feature4'])\n",
    "df['target'] = y\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_features = ['feature1', 'feature2', 'feature3']\n",
    "categorical_features = ['feature4']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "    # ,\n",
    "    # ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create full pipelines with different optimizers\n",
    "optimizers = ['lbfgs', 'sag', 'saga']\n",
    "pipelines = {}\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    pipelines[optimizer] = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(solver=optimizer, max_iter=1000, random_state=42))\n",
    "    ])\n",
    "\n",
    "# Train and evaluate each pipeline\n",
    "results = {}\n",
    "\n",
    "for optimizer, pipeline in pipelines.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    y_pred_proba_train = pipeline.predict_proba(X_train)\n",
    "    y_pred_proba_test = pipeline.predict_proba(X_test)\n",
    "    \n",
    "    results[optimizer] = {\n",
    "        'train_accuracy': accuracy_score(y_train, y_pred_train),\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'train_log_loss': log_loss(y_train, y_pred_proba_train),\n",
    "        'test_log_loss': log_loss(y_test, y_pred_proba_test)\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "print(\"Results with preprocessing:\")\n",
    "for optimizer, metrics in results.items():\n",
    "    print(f\"\\nOptimizer: {optimizer}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Compare with results without preprocessing\n",
    "pipelines_no_prep = {}\n",
    "\n",
    "\n",
    "# Experiment: Try a different scaling method (MinMaxScaler)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numeric_transformer_minmax = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
    "])\n",
    "\n",
    "preprocessor_minmax = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_minmax, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline_minmax = Pipeline([\n",
    "    ('preprocessor', preprocessor_minmax),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_minmax.fit(X_train, y_train)\n",
    "y_pred_test_minmax = pipeline_minmax.predict(X_test)\n",
    "y_pred_proba_test_minmax = pipeline_minmax.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nResults with MinMaxScaler:\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test_minmax):.4f}\")\n",
    "print(f\"Test log loss: {log_loss(y_test, y_pred_proba_test_minmax):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Questions to Consider\n",
    "\n",
    "1. How does preprocessing affect the performance of each optimizer?\n",
    "2. Which combination of preprocessing steps and optimizer yields the best performance? Why do you think this is?\n",
    "3. How does the training time compare between the preprocessed and non-preprocessed data for each optimizer?\n",
    "4. Are there any preprocessing steps that seem to be particularly important for this dataset?\n",
    "5. How might you further optimize this pipeline for better performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
